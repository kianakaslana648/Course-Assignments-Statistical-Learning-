## Lab Assignment

```{r}
library(tidyverse)
library(tree)
library(ISLR)
library(caret)
library(rpart)
library(rattle)
library(visNetwork)
library(randomForest)
```
### Problem 1: Implement decision tree from scratch

a) _Warm up_. Write a function called rss() that takes in a vector and returns the residual sum of squares (relative to the mean of the vector).

```{r}
rss <- function(x){
  return(sum((x-rep(mean(x),length(x)))**2))
}
```

b) _Best Split_. We're next going to write a function best_split() that will form the basis of a decision tree. This function will take in two arguments: a vector predictor variable (ie, x) and a vector target variable (ie, y). This function will identify the single location in the x domain that would yield the best split of the data, such that the two halves now each have least total RSS in the target variable. 

```{r}
best_split <- function(x, y){
  index = order(x)
  k = 1
  least = 0
  for(i in 1:(length(index)-1)){
    if(i == 1){
      least = rss(y[index[1:i]])+rss(y[index[(i+1):length(index)]])
    }else
    {
      temp = rss(y[index[1:i]])+rss(y[index[(i+1):length(index)]])
      if(temp < least){
        least = temp
        k = i
      }
    }
  }
  return(c(list(index[1:k]),list(index[(k+1):length(index)])))
}
```

Some things to think about:
 (i) If your input x vector has *n* data points, how many possible split locations are there? 
 (ii) You can accomplish this task by brute force. For every possible split location, split the data into two parts and compute the new total RSS. Then just return whichever split location was the optimal one. 
 
Make sure your function returns a few things (perhaps in a list): the location of the x split, the mean of y for each of the split parts, the improvement in RSS that was achieved by the split. 



c) _One Dimensional Data_ Here is a synthetic data set with one predictor and one response. Use your function to find out where the first split would be if $y$ is predicted from $x$ with a regression tree.

```{r}
x = seq(0,10,by = .01)
y0 = cos(x/4 + x^2/5)*x^2/20 + x 
y = y0 + rnorm(length(x))
mydf = data.frame(x=x,y=y)
rss0 = 1000*var(y)
plot(x,y)
#split0 <- bestsplit(x,y) 
```

```{r}
split = best_split(x,y)
ma = max(x[split[[1]]])
mi = min(x[split[[2]]])
c(ma, mi)
(ma + mi)/2
rss1 = rss(y[split[[1]]])+rss(y[split[[2]]])
rss0
rss1
```
What is the total RSS of y? What this RSS reduced to when you split the data? 

### Comment
The first split wound be any point in the set of c(ma, mi). From the rss before and after the split, we can see the total rss is greatly reduced.

d) _Growing the Tree_ 
Split the lower half again. Split the upper half again. What is the total RSS now?

```{r}
x1 = x[split[[1]]]
y1 = y[split[[1]]]
x2 = x[split[[2]]]
y2 = y[split[[2]]]

split1 = best_split(x1,y1)
split2 = best_split(x2,y2)

rss2 = rss(y1[split1[[1]]]) + rss(y1[split1[[2]]]) + rss(y2[split2[[1]]]) + rss(y2[split2[[2]]])
rss2

(max(x1[split1[[1]]])+min(x1[split1[[2]]]))/2
(max(x2[split2[[1]]])+min(x2[split2[[2]]]))/2
```

e) Try this all again with the same data but with R's tree() function. Make a plot of the tree. Do the first few splits agree with the results of your method?

```{r}
df_temp = data.frame(x=x,y=y)

tree1 = tree(y~., df_temp)
plot(tree1)
text(tree1)
```
### Comment
Yeah they match perfectly.

f) Now that we have a good understanding of how to do recursive binary splitting of a single variable, how do would you handle multiple predictors? 

### Comment
We can just apply the previous steps to each variable and have the best choice with the least total RSS.

### Problem 2: Complete one of two problems on abalone shells or airfoil data

Your group will be assigned one of the following problems to go over during lab. You will walkthrough your results at the end of class

#### 2a

_Work with Abalone Data_. These data may be found on the [UC Irvine website](http://archive.ics.uci.edu/ml/). They give various physical characteristics of about 4000 abalone shellfish. The data were collected in Tasmania in 1995. Make sure that the data are in your source directory.

a) We'll try to predict the number of Rings, using the other features. Train a linear model as a baseline case. What is the RSS or Rsquared?

```{r}
library(rstudioapi)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
load('abalone.RData')

model1 <- lm(Rings~., abalone)
pred1 = predict(model1, abalone)
sum((pred1-abalone$Rings)**2)
```

b) Now build a tree and plot it.  We can make the annotation of the tree look better by reducing the font size with the \texttt{cex} parameter. What is the depth of the tree? How many leaves does it have?

```{r}
tree2 <- tree(Rings~., abalone)
plot(tree2)
text(tree2,cex=0.6)
```
### Comment:
The depth is 5. The number of leaves is 10.

c) We can manually prune the tree to whatever depth we want. Use the function prune.tree() to simplify the tree so it only has 4 leaves. Visualize this tree.

### Comment:
```{r}
pr_tree1 <- prune.tree(tree2, best=4)
plot(pr_tree1)
text(pr_tree1,cex=0.6)
```

d) Which two continuous predictors seem to be highly predictive according to the tree? Draw a sketch of the feature space and the splits in the space, as well as the predicted number of Rings for each region.

### Comment:
ShellWeight and ShuckedWeight.

```{r}

library(ggplot2)
text_df = data.frame(x=c(0,0.1,0.27,0.75),y=c(0.75,0.75,0.75,0.75),label=c('5.687','8.189','10.650','12.820'))

ggplot(abalone,aes(x=ShellWeight, y=ShuckedWeight))+
  geom_point() + geom_vline(xintercept = 0.16775, linetype="dashed", color = "blue", size=1.5) + 
  geom_vline(xintercept = 0.05875, linetype="dashed", color = "blue", size=1.5) +
  geom_vline(xintercept = 0.37475, linetype="dashed", color = "blue", size=1.5) +
  geom_text(data=text_df, aes(x=x,y=y,label=label,color=factor(label)))
```

e) Decision trees have high variance. Split the Abalone data in half and train two trees (and don't worry about any extra pruning). Observe the differences between them, visualize the two different trees.

```{r}
ind = sample(1:dim(abalone)[1],dim(abalone)[1]/2)
first = abalone[ind,]
second = abalone[-ind,]

tree3 <- tree(Rings~., first)
tree4 <- tree(Rings~., second)

plot(tree3)
text(tree3,cex=0.6)

plot(tree4)
text(tree4,cex=0.6)
```
### Comment:
There is structure difference in the right ShellWeight part for the two trees.


#### 2b

Let's explore the airfoil data. We're going to try to predict the "Pressure" feature from the other features. Use one of the tree aggregation methods we have learned about (Random Forest, Boosting, Bagging)

(a) Start with some exploratory visualizations to see how the other features are related to Pressure. Feel free to use pairs(), or scatterplots, or boxplots. Do any features seem to be strongly predictive of Pressure?
```{r}
load('airfoil.RData')
pairs(airfoil)
```
There is evidence of relationship between Frequency and Pressure.

(b) Create a train-test split of the data.
```{r}
ind2 = sample(1:dim(airfoil)[1], 0.7 * dim(airfoil)[1])
train = airfoil[ind2,]
test = airfoil[-ind2,]
```

(c) Fit a linear model, observe the $R^2$ on the test set.
```{r}
model2 <- lm(Pressure~., train)
pred2 = predict(model2, test)
1-sum((pred2-test$Pressure)**2)/sum((test$Pressure-mean(test$Pressure))**2)
```
(d) Fit a single decision tree, one with large depth. Observe the $R^2$ on the test set. Make a plot of three if you wish.
```{r}
tree5 = tree(Pressure~.,train)
pred3 = predict(tree5, test)
1-sum((pred3-test$Pressure)**2)/sum((test$Pressure-mean(test$Pressure))**2)
plot(tree5)
text(tree5,cex=0.6)
```
(e) Fit a tree aggregation model, observe the $R^2$ on the test set.

```{r}
rf = randomForest(Pressure~.,data=train,mtry=2,importance=TRUE)
pred4 = predict(rf, test)
1-sum((pred4-test$Pressure)**2)/sum((test$Pressure-mean(test$Pressure))**2)
```